{
    "configuration": {
        "arch_config": "/scratch/user/sabuj.laskar/MeshFermat/src/SCALE-Sim/configs/express.cfg",
        "num_hmcs": 36,
        "num_vaults": 16,
        "mini_batch_size": 576,
        "network": "/scratch/user/sabuj.laskar/MeshFermat/src/SCALE-Sim/topologies/mlperf/AlphaGoZero.csv",
        "run_name": "AlphaGoZero",
        "outdir": "/scratch/user/sabuj.laskar/MeshFermat/results/HPCA2024/simba/outputs/AlphaGoZero",
        "dump": false,
        "allreduce": "ring",
        "booksim_config": "/scratch/user/sabuj.laskar/MeshFermat/src/booksim2/runfiles/mesh/anynet_mesh_36_200.cfg",
        "booksim_network": "mesh",
        "verbose": true,
        "only_compute": false,
        "only_allreduce": true,
        "only_reduce_scatter": false,
        "only_all_gather": false,
        "message_buffer_size": 32,
        "message_size": 8192,
        "sub_message_size": 8192,
        "synthetic_data_size": 0,
        "flits_per_packet": 16,
        "bandwidth": 200,
        "load_tree": false,
        "kary": 5,
        "radix": 4,
        "chunk_size": 0,
        "strict_schedule": true,
        "prioritize_schedule": true,
        "oracle_lockstep": false,
        "estimate_lockstep": false,
        "enable_logger": [],
        "save_link_utilization": false,
        "layer_by_layer": false,
        "layer_number": "0",
        "latency": 21,
        "per_message_time": 357,
        "layer_number_list": [
            0
        ],
        "pe_array_height": 32,
        "pe_array_width": 32,
        "ifmap_sram_size": 1048576,
        "filter_sram_size": 1048576,
        "ofmap_sram_size": 1048576,
        "ifmap_offset": 0,
        "filter_offset": 10000000,
        "ofmap_offset": 20000000,
        "ifmap_grad_offset": 40000000,
        "filter_grad_offset": 50000000,
        "ofmap_grad_offset": 30000000,
        "data_flow": "os",
        "logdir": "/scratch/user/sabuj.laskar/MeshFermat/results/HPCA2024/simba/outputs/AlphaGoZero",
        "nodes": 36
    },
    "results": {
        "performance": {
            "training": 0,
            "allreduce": {
                "computation": 3080,
                "pure_communication": 550676,
                "total": 553756
            },
            "total": 553756
        },
        "power": {
            "network": {
                "dynamic": 0.3270646479028729,
                "static": 6.608001860700779,
                "total": 6.935066508603652,
                "router": {
                    "dynamic": 0.3094786916725244,
                    "static": 6.592026109020779,
                    "total": 6.901504800693303
                },
                "link": {
                    "dynamic": 0.017585956230348452,
                    "static": 0.015975751679999987,
                    "total": 0.03356170791034844,
                    "flits": 942480
                }
            }
        }
    }
}