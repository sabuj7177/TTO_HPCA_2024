{
    "configuration": {
        "arch_config": "/scratch/user/sabuj.laskar/MeshFermat/src/SCALE-Sim/configs/express_16.cfg",
        "num_hmcs": 36,
        "num_vaults": 16,
        "mini_batch_size": 576,
        "network": "/scratch/user/sabuj.laskar/MeshFermat/src/SCALE-Sim/topologies/mlperf/Transformer.csv",
        "run_name": "Transformer",
        "outdir": "/scratch/user/sabuj.laskar/MeshFermat/results/HPCA2024/simba_new/outputs/Transformer",
        "dump": false,
        "allreduce": "ring2dn",
        "booksim_config": "/scratch/user/sabuj.laskar/MeshFermat/src/booksim2/runfiles/mesh/anynet_mesh_36_200.cfg",
        "booksim_network": "mesh",
        "verbose": true,
        "only_compute": false,
        "only_allreduce": true,
        "only_reduce_scatter": false,
        "only_all_gather": false,
        "message_buffer_size": 32,
        "message_size": 8192,
        "sub_message_size": 8192,
        "synthetic_data_size": 0,
        "flits_per_packet": 16,
        "bandwidth": 200,
        "load_tree": false,
        "kary": 5,
        "radix": 4,
        "chunk_size": 0,
        "strict_schedule": false,
        "prioritize_schedule": false,
        "oracle_lockstep": false,
        "estimate_lockstep": false,
        "enable_logger": [],
        "save_link_utilization": false,
        "layer_by_layer": false,
        "layer_number": "0",
        "latency": 21,
        "per_message_time": 357,
        "layer_number_list": [
            0
        ],
        "pe_array_height": 16,
        "pe_array_width": 16,
        "ifmap_sram_size": 524288,
        "filter_sram_size": 524288,
        "ofmap_sram_size": 524288,
        "ifmap_offset": 0,
        "filter_offset": 10000000,
        "ofmap_offset": 20000000,
        "ifmap_grad_offset": 40000000,
        "filter_grad_offset": 50000000,
        "ofmap_grad_offset": 30000000,
        "data_flow": "os",
        "logdir": "/scratch/user/sabuj.laskar/MeshFermat/results/HPCA2024/simba_new/outputs/Transformer",
        "nodes": 36
    },
    "results": {
        "performance": {
            "training": 0,
            "training_by_layer": null,
            "allreduce": {
                "computation": 588384,
                "pure_communication": 24902815,
                "total": 25491199
            },
            "total": 25491199
        },
        "power": {
            "network": {
                "dynamic": 0.42264131860927406,
                "static": 6.608001860700779,
                "total": 7.030643179310053,
                "router": {
                    "dynamic": 0.3922196697682375,
                    "static": 6.592026109020779,
                    "total": 6.984245778789016
                },
                "link": {
                    "dynamic": 0.03042164884103652,
                    "static": 0.015975751679999987,
                    "total": 0.04639740052103651,
                    "flits": 75051600
                }
            }
        }
    }
}